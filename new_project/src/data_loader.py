"""
ë°ì´í„°ì…‹ JSON ì„¤ì • íŒŒì¼ ë¡œë”
"""
import json
import os
import torch
from torch.utils.data import DataLoader, Dataset
from typing import Dict, Any, List, Optional, Union
import numpy as np

class GameOfLifeDataset(Dataset):
    """Game of Life ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¥¼ ìœ„í•œ Dataset í´ë˜ìŠ¤"""
    
    def __init__(self, data_files):
        self.data = []
        self.labels = []
        
        for file_path in data_files:
            if os.path.exists(file_path):
                file_data, file_labels = self._load_file(file_path)
                self.data.extend(file_data)
                self.labels.extend(file_labels)
                print("âœ… {}: {}ê°œ ìƒ˜í”Œ ë¡œë“œ".format(os.path.basename(file_path), len(file_data)))
            else:
                print("âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}".format(file_path))
        
        if len(self.data) == 0:
            print("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        else:
            print("\nğŸ“Š ì´ ë¡œë“œëœ ìƒ˜í”Œ: {}ê°œ".format(len(self.data)))
            print("ğŸ“Š ì „ì²´ ë ˆì´ë¸” ë²”ìœ„: {} ~ {}".format(min(self.labels), max(self.labels)))
    
    def _load_file(self, file_path):
        """ë‹¨ì¼ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (í˜•ì‹: [n] + 10x10 íŒ¨í„´ + ë ˆì´ë¸”)"""
        file_data = []
        file_labels = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            i = 0
            
            while i < len(lines):
                line = lines[i].strip()
                
                # [ìˆ«ì] í˜•ì‹ì˜ ìƒ˜í”Œ ì‹œì‘ í™•ì¸
                if line.startswith('[') and line.endswith(']'):
                    try:
                        # ìƒ˜í”Œ ë²ˆí˜¸ ì¶”ì¶œ
                        sample_num = int(line[1:-1])
                        
                        # 10x10 íŒ¨í„´ ë°ì´í„° ì½ê¸°
                        pattern_lines = []
                        for j in range(1, 11):  # ë‹¤ìŒ 10ì¤„
                            if i + j < len(lines):
                                pattern_line = lines[i + j].strip()
                                if len(pattern_line) == 10 and all(c in '01' for c in pattern_line):
                                    pattern_lines.append([int(bit) for bit in pattern_line])
                                else:
                                    break
                        
                        # ë ˆì´ë¸” ì½ê¸° (11ë²ˆì§¸ ì¤„)
                        if i + 11 < len(lines) and len(pattern_lines) == 10:
                            label_line = lines[i + 11].strip()
                            if label_line.isdigit():
                                label = int(label_line)
                                
                                # 10x10ì„ 50x50ìœ¼ë¡œ í™•ì¥ (ì¤‘ì•™ì— ë°°ì¹˜)
                                expanded_pattern = np.zeros((50, 50), dtype=np.float32)
                                start_row = (50 - 10) // 2  # 20
                                start_col = (50 - 10) // 2  # 20
                                
                                for row_idx, row in enumerate(pattern_lines):
                                    for col_idx, value in enumerate(row):
                                        expanded_pattern[start_row + row_idx, start_col + col_idx] = float(value)
                                
                                # ë°ì´í„° ì €ì¥
                                file_data.append(expanded_pattern)
                                file_labels.append(label)
                        
                        i += 12  # ë‹¤ìŒ ìƒ˜í”Œë¡œ ì´ë™ ([n] + 10ì¤„ íŒ¨í„´ + 1ì¤„ ë ˆì´ë¸”)
                    except (ValueError, IndexError):
                        i += 1
                else:
                    i += 1
        
        except Exception as e:
            print("íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {}: {}".format(file_path, e))
        
        return file_data, file_labels
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        grid = torch.FloatTensor(self.data[idx])
        label = self.labels[idx]
        
        # ë ˆì´ë¸”ì„ 10ë¹„íŠ¸ ì´ì§„ìˆ˜ë¡œ ë³€í™˜
        binary_target = torch.zeros(10, dtype=torch.float32)
        for i in range(10):
            if label & (1 << (9-i)):
                binary_target[i] = 1.0
        
        return grid, binary_target

class DatasetLoader:
    """JSON ì„¤ì • íŒŒì¼ì„ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ ë¡œë”"""
    
    def __init__(self, config_path: str):
        self.config_path = config_path
        self.config = {}
        self.load_config()
    
    def _resolve_file_path(self, path: str) -> str:
        """íŒŒì¼ ê²½ë¡œë¥¼ í•´ê²°í•˜ëŠ” ë‹¨ìˆœí™”ëœ ë°©ì‹"""
        if os.path.isabs(path):
            # ì ˆëŒ€ ê²½ë¡œì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            return path
        
        # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° 3ë‹¨ê³„ë¡œë§Œ ì‹œë„
        # 1. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€
        abs_path1 = os.path.join(os.getcwd(), path)
        if os.path.exists(abs_path1):
            return abs_path1
            
        # 2. config íŒŒì¼ ê¸°ì¤€
        config_dir = os.path.dirname(os.path.abspath(self.config_path))
        abs_path2 = os.path.join(config_dir, path)
        if os.path.exists(abs_path2):
            return abs_path2
            
        # 3. í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ train_data í´ë”ì—ì„œ íŒŒì¼ëª…ìœ¼ë¡œ ì°¾ê¸°
        filename = os.path.basename(path)
        current = os.getcwd()
        for _ in range(3):  # ìµœëŒ€ 3ë‹¨ê³„ ìƒìœ„ê¹Œì§€
            train_data_path = os.path.join(current, 'train_data', filename)
            if os.path.exists(train_data_path):
                return train_data_path
            parent = os.path.dirname(current)
            if parent == current:
                break
            current = parent
        
        # íŒŒì¼ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì²« ë²ˆì§¸ ê²½ë¡œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ë°˜í™˜
        print("âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}".format(path))
        return abs_path1
    
    def load_config(self):
        """JSON ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            if not os.path.exists(self.config_path):
                print("âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}".format(self.config_path))
                return False
                
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
                
            print("âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì„±ê³µ: {}".format(self.config_path))
            return True
            
        except json.JSONDecodeError as e:
            print("âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {}".format(e))
            return False
        except Exception as e:
            print("âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
            return False
    
    def get_dataset_names(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ì´ë¦„ ëª©ë¡ ë°˜í™˜"""
        if 'datasets' in self.config:
            return list(self.config['datasets'].keys())
        return []
    
    def get_dataset_info(self, dataset_name: str) -> Optional[Dict]:
        """ë°ì´í„°ì…‹ ì •ë³´ ë°˜í™˜"""
        if 'datasets' in self.config and dataset_name in self.config['datasets']:
            return self.config['datasets'][dataset_name]
        return None
    
    def create_dataloader(self, dataset_name: str, batch_size: int = 32, 
                         shuffle: bool = True, num_workers: int = 4) -> Optional[DataLoader]:
        """ë°ì´í„°ì…‹ ë¡œë” ìƒì„±"""
        dataset_info = self.get_dataset_info(dataset_name)
        if not dataset_info:
            print("âŒ ë°ì´í„°ì…‹ '{}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.".format(dataset_name))
            return None
        
        if dataset_info['type'] != 'simulation_files':
            print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹ íƒ€ì…: {}".format(dataset_info['type']))
            return None
        
        # íŒŒì¼ ê²½ë¡œ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (ë‹¨ìˆœí™”ëœ 3ë‹¨ê³„ ë°©ì‹)
        file_paths = []
        
        for path in dataset_info['paths']:
            abs_path = self._resolve_file_path(path)
            file_paths.append(abs_path)
        
        print("ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”©: {} ({}ê°œ íŒŒì¼)".format(dataset_info['name'], len(file_paths)))
        
        # ì²« ë²ˆì§¸ íŒŒì¼ì˜ ìœ„ì¹˜ë§Œ í‘œì‹œ
        if file_paths and os.path.exists(file_paths[0]):
            print("ğŸ“ ë°ì´í„° íŒŒì¼ ìœ„ì¹˜: {}".format(os.path.dirname(file_paths[0])))
        
        # ë°ì´í„°ì…‹ ìƒì„±
        dataset = GameOfLifeDataset(file_paths)
        
        if len(dataset) == 0:
            print("âŒ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None
        
        # DataLoader ìƒì„±
        try:
            dataloader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=shuffle,
                num_workers=num_workers,
                pin_memory=True if torch.cuda.is_available() else False
            )
            print("ğŸš€ DataLoader ìƒì„± ì™„ë£Œ: batch_size={}, ì´ ë°°ì¹˜ ìˆ˜={}".format(batch_size, len(dataloader)))
            return dataloader
            
        except Exception as e:
            print("âŒ DataLoader ìƒì„± ì‹¤íŒ¨: {}".format(e))
            return None
    
    def print_available_datasets(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¶œë ¥"""
        dataset_names = self.get_dataset_names()
        if not dataset_names:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\nğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:")
        for i, name in enumerate(dataset_names, 1):
            info = self.get_dataset_info(name)
            if info:
                print("  {}. {} - {}".format(i, name, info['name']))
                print("     ì˜ˆìƒ ìƒ˜í”Œ: {:,}ê°œ".format(info['expected_samples']))
                print("     ì„¤ëª…: {}".format(info['description']))
        print()

def load_dataset_from_files(file_paths: Union[str, List[str]], 
                           batch_size: int = 32, shuffle: bool = True, 
                           num_workers: int = 4) -> Optional[DataLoader]:
    """íŒŒì¼ ê²½ë¡œë¡œë¶€í„° ì§ì ‘ ë°ì´í„°ì…‹ ë¡œë“œ"""
    if isinstance(file_paths, str):
        file_paths = [file_paths]
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    missing_files = [path for path in file_paths if not os.path.exists(path)]
    if missing_files:
        print("âŒ ë‹¤ìŒ íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:")
        for f in missing_files:
            print("   - {}".format(f))
        return None
    
    print("ğŸ“‚ ì§ì ‘ íŒŒì¼ ë¡œë“œ: {}ê°œ íŒŒì¼".format(len(file_paths)))
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = GameOfLifeDataset(file_paths)
    if len(dataset) == 0:
        return None
    
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, 
                     num_workers=num_workers, pin_memory=torch.cuda.is_available())