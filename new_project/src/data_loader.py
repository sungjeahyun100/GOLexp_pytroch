"""
ë°ì´í„°ì…‹ JSON ì„¤ì • íŒŒì¼ ë¡œë”
"""
import json
import os
import torch
from torch.utils.data import DataLoader, Dataset
from typing import Dict, Any, List, Optional, Union
import numpy as np

class GameOfLifeDataset(Dataset):
    """Game of Life ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¥¼ ìœ„í•œ Dataset í´ë˜ìŠ¤"""
    
    def __init__(self, data_files):
        self.data = []
        self.labels = []
        
        for file_path in data_files:
            if os.path.exists(file_path):
                file_data, file_labels = self._load_file(file_path)
                self.data.extend(file_data)
                self.labels.extend(file_labels)
                print("âœ… {}: {}ê°œ ìƒ˜í”Œ ë¡œë“œ".format(os.path.basename(file_path), len(file_data)))
            else:
                print("âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}".format(file_path))
        
        if len(self.data) == 0:
            print("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        else:
            print("\nğŸ“Š ì´ ë¡œë“œëœ ìƒ˜í”Œ: {}ê°œ".format(len(self.data)))
            print("ğŸ“Š ì „ì²´ ë ˆì´ë¸” ë²”ìœ„: {} ~ {}".format(min(self.labels), max(self.labels)))
    
    def _load_file(self, file_path):
        """ë‹¨ì¼ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (í˜•ì‹: [n] + 10x10 íŒ¨í„´ + ë ˆì´ë¸”)"""
        file_data = []
        file_labels = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            i = 0
            
            while i < len(lines):
                line = lines[i].strip()
                
                # [ìˆ«ì] í˜•ì‹ì˜ ìƒ˜í”Œ ì‹œì‘ í™•ì¸
                if line.startswith('[') and line.endswith(']'):
                    try:
                        # ìƒ˜í”Œ ë²ˆí˜¸ ì¶”ì¶œ
                        sample_num = int(line[1:-1])
                        
                        # 10x10 íŒ¨í„´ ë°ì´í„° ì½ê¸°
                        pattern_lines = []
                        for j in range(1, 11):  # ë‹¤ìŒ 10ì¤„
                            if i + j < len(lines):
                                pattern_line = lines[i + j].strip()
                                if len(pattern_line) == 10 and all(c in '01' for c in pattern_line):
                                    pattern_lines.append([int(bit) for bit in pattern_line])
                                else:
                                    break
                        
                        # ë ˆì´ë¸” ì½ê¸° (11ë²ˆì§¸ ì¤„)
                        if i + 11 < len(lines) and len(pattern_lines) == 10:
                            label_line = lines[i + 11].strip()
                            if label_line.isdigit():
                                label = int(label_line)
                                
                                # 10x10ì„ 50x50ìœ¼ë¡œ í™•ì¥ (ì¤‘ì•™ì— ë°°ì¹˜)
                                expanded_pattern = np.zeros((50, 50), dtype=np.float32)
                                start_row = (50 - 10) // 2  # 20
                                start_col = (50 - 10) // 2  # 20
                                
                                for row_idx, row in enumerate(pattern_lines):
                                    for col_idx, value in enumerate(row):
                                        expanded_pattern[start_row + row_idx, start_col + col_idx] = float(value)
                                
                                # ë°ì´í„° ì €ì¥
                                file_data.append(expanded_pattern)
                                file_labels.append(label)
                        
                        i += 12  # ë‹¤ìŒ ìƒ˜í”Œë¡œ ì´ë™ ([n] + 10ì¤„ íŒ¨í„´ + 1ì¤„ ë ˆì´ë¸”)
                    except (ValueError, IndexError):
                        i += 1
                else:
                    i += 1
        
        except Exception as e:
            print("íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {}: {}".format(file_path, e))
        
        return file_data, file_labels
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        grid = torch.FloatTensor(self.data[idx])
        label = self.labels[idx]
        
        # ë ˆì´ë¸”ì„ 10ë¹„íŠ¸ ì´ì§„ìˆ˜ë¡œ ë³€í™˜
        binary_target = torch.zeros(10, dtype=torch.float32)
        for i in range(10):
            if label & (1 << (9-i)):
                binary_target[i] = 1.0
        
        return grid, binary_target

class DatasetLoader:
    """JSON ì„¤ì • íŒŒì¼ì„ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ ë¡œë”"""
    
    def __init__(self, config_path: str):
        self.config_path = config_path
        self.config = {}
        self.load_config()
    
    def _find_git_or_project_root(self, start_path: str) -> Optional[str]:
        """Git ë£¨íŠ¸ë‚˜ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤"""
        current = os.path.abspath(start_path)
        
        # ìµœëŒ€ 5ë‹¨ê³„ê¹Œì§€ ìƒìœ„ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
        for _ in range(5):
            # .git í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸
            if os.path.exists(os.path.join(current, '.git')):
                return current
                
            # train_data í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ í‘œì‹œ)
            if os.path.exists(os.path.join(current, 'train_data')):
                return current
                
            # CMakeLists.txtë‚˜ README.mdê°€ ìˆìœ¼ë©´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ê°„ì£¼
            if (os.path.exists(os.path.join(current, 'CMakeLists.txt')) or 
                os.path.exists(os.path.join(current, 'README.md'))):
                return current
            
            parent = os.path.dirname(current)
            if parent == current:  # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì— ë„ë‹¬
                break
            current = parent
        
        return None
    
    def load_config(self):
        """JSON ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            if not os.path.exists(self.config_path):
                print("âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}".format(self.config_path))
                return False
                
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
                
            print("âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì„±ê³µ: {}".format(self.config_path))
            return True
            
        except json.JSONDecodeError as e:
            print("âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {}".format(e))
            return False
        except Exception as e:
            print("âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
            return False
    
    def get_dataset_names(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ì´ë¦„ ëª©ë¡ ë°˜í™˜"""
        if 'datasets' in self.config:
            return list(self.config['datasets'].keys())
        return []
    
    def get_dataset_info(self, dataset_name: str) -> Optional[Dict]:
        """ë°ì´í„°ì…‹ ì •ë³´ ë°˜í™˜"""
        if 'datasets' in self.config and dataset_name in self.config['datasets']:
            return self.config['datasets'][dataset_name]
        return None
    
    def create_dataloader(self, dataset_name: str, batch_size: int = 32, 
                         shuffle: bool = True, num_workers: int = 4) -> Optional[DataLoader]:
        """ë°ì´í„°ì…‹ ë¡œë” ìƒì„±"""
        dataset_info = self.get_dataset_info(dataset_name)
        if not dataset_info:
            print("âŒ ë°ì´í„°ì…‹ '{}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.".format(dataset_name))
            return None
        
        if dataset_info['type'] != 'simulation_files':
            print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹ íƒ€ì…: {}".format(dataset_info['type']))
            return None
        
        # íŒŒì¼ ê²½ë¡œ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (ë” ê²¬ê³ í•œ ë°©ì‹)
        # 1. config íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ í•´ì„
        config_dir = os.path.dirname(os.path.abspath(self.config_path))
        # 2. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë„ ê³ ë ¤ (train.pyê°€ ì‹¤í–‰ë˜ëŠ” ìœ„ì¹˜)
        current_dir = os.getcwd()
        
        file_paths = []
        
        for path in dataset_info['paths']:
            if os.path.isabs(path):
                # ì ˆëŒ€ ê²½ë¡œì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                abs_path = path
            else:
                # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„
                abs_path = None
                
                # ë°©ë²• 1: config íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ í•´ì„
                try_path1 = os.path.join(config_dir, path)
                
                # ë°©ë²• 2: í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ í•´ì„
                try_path2 = os.path.join(current_dir, path)
                
                # ë°©ë²• 3: í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ train_data ì°¾ê¸°
                # config_dirì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ì—ì„œ train_data í´ë” ì°¾ê¸°
                project_root = os.path.dirname(config_dir)
                filename = os.path.basename(path)
                try_path3 = os.path.join(project_root, 'train_data', filename)
                
                # ë°©ë²• 4-6: íŒŒì¼ëª…ë§Œ ìˆëŠ” ê²½ìš°, train_data í´ë”ë¥¼ ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ ì°¾ê¸°
                if '/' not in path and '\\' not in path:  # íŒŒì¼ëª…ë§Œ ìˆëŠ” ê²½ìš°
                    # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ train_data ì°¾ê¸°
                    try_path4 = os.path.join(current_dir, 'train_data', path)
                    # ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œ train_data ì°¾ê¸° 
                    try_path5 = os.path.join(os.path.dirname(current_dir), 'train_data', path)
                    # ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ìƒìœ„ì—ì„œ train_data ì°¾ê¸° (git ë£¨íŠ¸ ë“±)
                    try_path6 = os.path.join(os.path.dirname(os.path.dirname(current_dir)), 'train_data', path)
                    
                    # ë°©ë²• 7: config íŒŒì¼ ê²½ë¡œì—ì„œ git ë£¨íŠ¸ ì°¾ê¸°
                    git_root = self._find_git_or_project_root(config_dir)
                    try_path7 = os.path.join(git_root, 'train_data', path) if git_root else None
                else:
                    try_path4 = try_path5 = try_path6 = try_path7 = None
                
                # ì¡´ì¬í•˜ëŠ” ê²½ë¡œ ì°¾ê¸°
                candidates = [try_path1, try_path2, try_path3]
                if try_path4:
                    candidates.append(try_path4)
                if try_path5:
                    candidates.append(try_path5)
                if try_path6:
                    candidates.append(try_path6)
                if try_path7:
                    candidates.append(try_path7)
                    
                for candidate in candidates:
                    if candidate and os.path.exists(candidate):
                        abs_path = candidate
                        break
                
                # ë””ë²„ê¹…ì„ ìœ„í•œ ì •ë³´ ì¶œë ¥ (íŒŒì¼ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°ë§Œ)
                if abs_path is None:
                    print("ğŸ” íŒŒì¼ '{}' ê²½ë¡œ íƒìƒ‰ ê²°ê³¼:".format(os.path.basename(path)))
                    for i, candidate in enumerate(candidates, 1):
                        if candidate:
                            exists = "âœ…" if os.path.exists(candidate) else "âŒ"
                            print("   {}. {} {}".format(i, exists, candidate))
                    abs_path = try_path1  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì²« ë²ˆì§¸ ì‹œë„ ì‚¬ìš©
                else:
                    # ì„±ê³µì ìœ¼ë¡œ ì°¾ì•˜ì„ ë•ŒëŠ” ê°„ë‹¨í•œ ë¡œê·¸ë§Œ (ì²« ë²ˆì§¸ íŒŒì¼ì— ëŒ€í•´ì„œë§Œ)
                    if path == dataset_info['paths'][0]:
                        print("ğŸ“ ë°ì´í„° íŒŒì¼ ìœ„ì¹˜: {}".format(os.path.dirname(abs_path)))
            
            file_paths.append(abs_path)
        
        print("ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”©: {} ({}ê°œ íŒŒì¼)".format(dataset_info['name'], len(file_paths)))
        
        # ë°ì´í„°ì…‹ ìƒì„±
        dataset = GameOfLifeDataset(file_paths)
        
        if len(dataset) == 0:
            print("âŒ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None
        
        # DataLoader ìƒì„±
        try:
            dataloader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=shuffle,
                num_workers=num_workers,
                pin_memory=True if torch.cuda.is_available() else False
            )
            print("ğŸš€ DataLoader ìƒì„± ì™„ë£Œ: batch_size={}, ì´ ë°°ì¹˜ ìˆ˜={}".format(batch_size, len(dataloader)))
            return dataloader
            
        except Exception as e:
            print("âŒ DataLoader ìƒì„± ì‹¤íŒ¨: {}".format(e))
            return None
    
    def print_available_datasets(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¶œë ¥"""
        dataset_names = self.get_dataset_names()
        if not dataset_names:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\nğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:")
        for i, name in enumerate(dataset_names, 1):
            info = self.get_dataset_info(name)
            if info:
                print("  {}. {} - {}".format(i, name, info['name']))
                print("     ì˜ˆìƒ ìƒ˜í”Œ: {:,}ê°œ".format(info['expected_samples']))
                print("     ì„¤ëª…: {}".format(info['description']))
        print()

def load_dataset_from_files(file_paths: Union[str, List[str]], 
                           batch_size: int = 32, shuffle: bool = True, 
                           num_workers: int = 4) -> Optional[DataLoader]:
    """íŒŒì¼ ê²½ë¡œë¡œë¶€í„° ì§ì ‘ ë°ì´í„°ì…‹ ë¡œë“œ"""
    if isinstance(file_paths, str):
        file_paths = [file_paths]
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    missing_files = [path for path in file_paths if not os.path.exists(path)]
    if missing_files:
        print("âŒ ë‹¤ìŒ íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:")
        for f in missing_files:
            print("   - {}".format(f))
        return None
    
    print("ğŸ“‚ ì§ì ‘ íŒŒì¼ ë¡œë“œ: {}ê°œ íŒŒì¼".format(len(file_paths)))
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = GameOfLifeDataset(file_paths)
    if len(dataset) == 0:
        return None
    
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, 
                     num_workers=num_workers, pin_memory=torch.cuda.is_available())