# -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("ì‚¬ìš© ë””ë°”ì´ìŠ¤: {}".format(device))

def getActive(activeName, input):
    if activeName == 'tanh':
        return F.tanh(input)
    elif activeName == 'relu':
        return F.relu(input)
    elif activeName == 'sigmoid':
        return F.sigmoid(input)
    elif activeName == 'elu':
        return F.elu(input)
    elif activeName == 'selu':
        return F.selu(input)
    elif activeName == 'lrelu':
        return F.leaky_relu(input)
    elif activeName == 'swish':
        return F.silu(input)
    else:
        return F.tanh(input)

class DenseLayer(nn.Module):
    def __init__(self, inputLayer, hidden1Layer, hidden2Layer, hidden3Layer, outputLayer, activate):
        super(DenseLayer, self).__init__()
        self.fc1 = nn.Linear(inputLayer, hidden1Layer)
        self.fc2 = nn.Linear(hidden1Layer, hidden2Layer)
        self.fc3 = nn.Linear(hidden2Layer, hidden3Layer)
        self.fc4 = nn.Linear(hidden3Layer, outputLayer)
        self.fc_act = activate

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = getActive(self.fc_act, self.fc1(x))
        x = getActive(self.fc_act, self.fc2(x))
        x = getActive(self.fc_act, self.fc3(x))
        x = torch.sigmoid(self.fc4(x))
        return x

class cnnLayer(nn.Module):
    def __init__(self, inputSize, hidden1Size, hidden2Size, outputSize, activate, stride, use_bias):
        super(cnnLayer, self).__init__()
        
        # CNN ë ˆì´ì–´ë“¤ (3ê°œ)
        self.conv1 = nn.Conv2d(1, hidden1Size, kernel_size=3, stride=stride, padding=1, bias=use_bias)
        self.conv2 = nn.Conv2d(hidden1Size, hidden2Size, kernel_size=3, stride=stride, padding=1, bias=use_bias)
        self.conv3 = nn.Conv2d(hidden2Size, hidden2Size*2, kernel_size=3, stride=stride, padding=1, bias=use_bias)
        
        # í’€ë§ ë ˆì´ì–´
        self.pool = nn.MaxPool2d(2, 2)
        
        # ë°°ì¹˜ ì •ê·œí™”
        self.bn1 = nn.BatchNorm2d(hidden1Size, affine=False)
        self.bn2 = nn.BatchNorm2d(hidden2Size, affine=False)
        self.bn3 = nn.BatchNorm2d(hidden2Size*2, affine=False)
        
        # FC layer ì…ë ¥ í¬ê¸° ê³„ì‚°
        self._calculate_fc_input_size(inputSize, hidden2Size*2)
        
        # Dense ë ˆì´ì–´ë“¤ (5ê°œ)
        self.fc1 = nn.Linear(self.fc_input_size, 1024, bias=use_bias)
        self.fc2 = nn.Linear(1024, 512, bias=use_bias)
        self.fc3 = nn.Linear(512, 256, bias=use_bias)
        self.fc4 = nn.Linear(256, 128, bias=use_bias)
        self.fc5 = nn.Linear(128, outputSize, bias=use_bias)
        
        # í™œì„±í™” í•¨ìˆ˜ ì €ì¥
        self.fc_act = activate
    
    def _calculate_fc_input_size(self, inputSize, final_channels):
        """FC layer ì…ë ¥ í¬ê¸° ê³„ì‚°"""
        # 10x10 â†’ pool â†’ 5x5 â†’ pool â†’ 2x2 â†’ pool â†’ 1x1
        # 3ë²ˆì˜ pooling (2x2)ìœ¼ë¡œ 8ë°° ê°ì†Œ
        size_after_conv = max(1, inputSize // 8)
        self.fc_input_size = final_channels * size_after_conv * size_after_conv
    
    def forward(self, x):
        # ì…ë ¥ í˜•íƒœ ì¡°ì •: (batch, height, width) â†’ (batch, 1, height, width)
        if x.dim() == 3:
            x = x.unsqueeze(1)
        
        # CNN ë ˆì´ì–´ë“¤ (3ê°œ)
        x = self.pool(getActive(self.fc_act, self.bn1(self.conv1(x))))
        x = self.pool(getActive(self.fc_act, self.bn2(self.conv2(x))))
        x = self.pool(getActive(self.fc_act, self.bn3(self.conv3(x))))
        
        # Flatten
        x = x.view(x.size(0), -1)
        
        # Dense ë ˆì´ì–´ë“¤ (5ê°œ)
        x = getActive(self.fc_act, self.fc1(x))
        x = getActive(self.fc_act, self.fc2(x))
        x = getActive(self.fc_act, self.fc3(x))
        x = getActive(self.fc_act, self.fc4(x))
        x = torch.sigmoid(self.fc5(x))  # ë§ˆì§€ë§‰ ë ˆì´ì–´ëŠ” í™œì„±í™” í•¨ìˆ˜ ì ìš© ì•ˆ í•¨
        
        return x

def save_model(model, filepath, model_info=None, optimizer=None, epoch=0):
    """ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ ì •ë³´ ì €ì¥ (ì˜µí‹°ë§ˆì´ì € ìƒíƒœ í¬í•¨)"""
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    save_dict = {
        'model_state_dict': model.state_dict(),
        'model_info': model_info or {},
        'epoch': epoch
    }
    
    # ì˜µí‹°ë§ˆì´ì € ìƒíƒœë„ ì €ì¥ (ì¶”ê°€ í•™ìŠµìš©)
    if optimizer is not None:
        save_dict['optimizer_state_dict'] = optimizer.state_dict()
    
    torch.save(save_dict, filepath)
    print(f"ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")

def load_model(model_path, model_class, model_params, load_optimizer=False, lr=0.001):
    """ëª¨ë¸ ë¡œë“œ (ì¶”ê°€ í•™ìŠµ ì§€ì›)"""
    try:
        if torch.cuda.is_available():
            checkpoint = torch.load(model_path)
        else:
            checkpoint = torch.load(model_path, map_location='cpu')
        
        # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        model_info = checkpoint.get('model_info', {})
        saved_epoch = checkpoint.get('epoch', 0)
        
        # ëª¨ë¸ ìƒì„±
        model = model_class(**model_params).to(device)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        optimizer = optim.AdamW(model.parameters(), lr=lr)
        if load_optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print("ì˜µí‹°ë§ˆì´ì € ìƒíƒœë„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤ (Epoch {}ë¶€í„° ê³„ì†)".format(saved_epoch))
        
        print("ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {}".format(model_path))
        print("ëª¨ë¸ ì •ë³´: {}".format(model_info))
        print("ì €ì¥ëœ Epoch: {}".format(saved_epoch))
        
        return model, optimizer, model_info, saved_epoch
        
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None, None, 0

def continue_training(model_path, train_loader, additional_epochs=100, lr=0.001, reset_optimizer=True, dataset_info=None):
    """ê¸°ì¡´ ëª¨ë¸ì„ ë¡œë“œí•´ì„œ ì¶”ê°€ í›ˆë ¨"""
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ì˜ (ì €ì¥ëœ ì •ë³´ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŒ)
    model_params = {
        'inputSize': 10,
        'hidden1Size': 32,
        'hidden2Size': 64,
        'outputSize': 10,
        'activate': 'swish',
        'stride': 1,
        'use_bias': False
    }
    
    # ëª¨ë¸ ë¡œë“œ (ì˜µí‹°ë§ˆì´ì €ëŠ” ì¡°ê±´ë¶€)
    model, optimizer, model_info, start_epoch = load_model(
        model_path, cnnLayer, model_params, load_optimizer=not reset_optimizer, lr=lr
    )
    
    if model is None:
        print("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨.")
        return None, None
    
    # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” (ìƒˆë¡œìš´ í•™ìŠµë¥ ë¡œ)
    if reset_optimizer:
        optimizer = optim.AdamW(model.parameters(), lr=lr)
        print(f"ğŸ”„ ì˜µí‹°ë§ˆì´ì € ìƒˆë¡œ ì´ˆê¸°í™” (í•™ìŠµë¥ : {lr})")
    elif optimizer is None:
        # ì˜µí‹°ë§ˆì´ì € ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìƒˆë¡œ ìƒì„±
        optimizer = optim.AdamW(model.parameters(), lr=lr)
        print(f"âš ï¸ ì˜µí‹°ë§ˆì´ì € ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„± (í•™ìŠµë¥ : {lr})")
    
    print(f"\nğŸ”„ ì¶”ê°€ í›ˆë ¨ ì‹œì‘: Epoch {start_epoch+1} ~ {start_epoch+additional_epochs}")
    
    # ì¶”ê°€ í›ˆë ¨
    criterion = nn.BCELoss()
    
    model.train()
    for epoch in range(additional_epochs):
        current_epoch = start_epoch + epoch + 1
        total_loss = 0
        
        for batch_x, batch_y in train_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        if (epoch + 1) % 20 == 0:
            print(f'Epoch [{current_epoch}/{start_epoch+additional_epochs}], Loss: {total_loss/len(train_loader):.4f}')
    
    # ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ ì •ë³´
    if model_info is not None:
        updated_info = model_info.copy()
        updated_info['total_epochs'] = start_epoch + additional_epochs
        updated_info['additional_training'] = True
        
        # ë°ì´í„°ì…‹ ì •ë³´ ì—…ë°ì´íŠ¸ (ì „ë‹¬ë°›ì€ ê²½ìš°)
        if dataset_info is not None:
            updated_info['training_samples'] = dataset_info['samples']
            updated_info['label_range'] = dataset_info['label_range']
            updated_info['dataset_type'] = dataset_info['dataset_type']
            
    else:
        updated_info = {'total_epochs': start_epoch + additional_epochs, 'additional_training': True}
        if dataset_info is not None:
            updated_info.update(dataset_info)
    
    # ìƒˆ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥ (ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸° ë°©ì§€)
    new_model_path = model_path.replace('.pth', f'_continued_{start_epoch+additional_epochs}.pth')
    save_model(model, new_model_path, updated_info, optimizer, start_epoch + additional_epochs)
    
    return model, new_model_path

class NewBinaryDataset(Dataset):
    def __init__(self, data_file_list):
        self.data = []
        self.labels = []
        if isinstance(data_file_list, str):
            # ë‹¨ì¼ íŒŒì¼ì¸ ê²½ìš°
            self.load_single_file(data_file_list)
        else:
            # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            self.load_multiple_files(data_file_list)
    
    def load_single_file(self, data_file):
        """ë‹¨ì¼ íŒŒì¼ ë¡œë“œ"""
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            i = 0
            sample_count = 0
            
            while i < len(lines):
                line = lines[i].strip()
                
                # [ìˆ«ì] í˜•ì‹ì˜ ìƒ˜í”Œ ì‹œì‘ í™•ì¸
                if line.startswith('[') and line.endswith(']'):
                    try:
                        # ìƒ˜í”Œ ë²ˆí˜¸ ì¶”ì¶œ
                        sample_num = int(line[1:-1])
                        
                        # 10x10 íŒ¨í„´ ë°ì´í„° ì½ê¸°
                        pattern_lines = []
                        for j in range(1, 11):  # ë‹¤ìŒ 10ì¤„
                            if i + j < len(lines):
                                pattern_line = lines[i + j].strip()
                                if len(pattern_line) == 10 and all(c in '01' for c in pattern_line):
                                    pattern_lines.append([int(bit) for bit in pattern_line])
                                else:
                                    break
                        
                        # ë ˆì´ë¸” ì½ê¸° (11ë²ˆì§¸ ì¤„)
                        if i + 11 < len(lines) and len(pattern_lines) == 10:
                            label_line = lines[i + 11].strip()
                            if label_line.isdigit():
                                label = int(label_line)
                                
                                # ë°ì´í„° ì €ì¥
                                self.data.append(pattern_lines)
                                self.labels.append(label)
                                sample_count += 1
                        
                        i += 12  # ë‹¤ìŒ ìƒ˜í”Œë¡œ ì´ë™ ([n] + 10ì¤„ íŒ¨í„´ + 1ì¤„ ë ˆì´ë¸”)
                        
                    except ValueError:
                        i += 1
                else:
                    i += 1
            
            print(f"âœ… {os.path.basename(data_file)}: {sample_count}ê°œ ìƒ˜í”Œ ë¡œë“œ")
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {data_file}: {e}")
    
    def load_multiple_files(self, file_list):
        """ì—¬ëŸ¬ íŒŒì¼ì„ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ í•©ì¹¨"""
        total_samples = 0
        
        for file_path in file_list:
            if os.path.exists(file_path):
                initial_count = len(self.data)
                self.load_single_file(file_path)
                added_count = len(self.data) - initial_count
                total_samples += added_count
            else:
                print(f"âŒ íŒŒì¼ ì—†ìŒ: {file_path}")
        
        print(f"\nğŸ“Š ì´ ë¡œë“œëœ ìƒ˜í”Œ: {total_samples}ê°œ")
        if self.labels:
            print(f"ğŸ“Š ì „ì²´ ë ˆì´ë¸” ë²”ìœ„: {min(self.labels)} ~ {max(self.labels)}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        # 10x10 íŒ¨í„´ì„ í…ì„œë¡œ ë³€í™˜
        data = torch.tensor(self.data[idx], dtype=torch.float32)
        
        # 10bit ë³€í™˜
        label_value = self.labels[idx]
        label_10bit = [(label_value >> i) & 1 for i in range(9, -1, -1)]
        label = torch.tensor(label_10bit, dtype=torch.float32)
        
        return data, label

def create_dataloader_from_files(data_files, batch_size=64, shuffle=True, num_workers=4):
    """íŒŒì¼(ë“¤)ì—ì„œ ë°ì´í„°ë¡œë” ìƒì„±"""
    dataset = NewBinaryDataset(data_files)
    
    if len(dataset) == 0:
        print("âŒ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return None, None
    
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)
    
    return dataloader, dataset

def train_model(model, train_loader, epochs=100):
    criterion = nn.BCELoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.001)
    
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        if (epoch + 1) % 20 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')

def predict_to_number(model, data_tensor):
    model.eval()
    with torch.no_grad():
        # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
        data_tensor = data_tensor.to(device)
        output = model(data_tensor)
        binary_output = (output > 0.5).float()
        # GPUì—ì„œ CPUë¡œ ì´ë™ í›„ ê³„ì‚°
        binary_output = binary_output.cpu()
        result = 0
        for i in range(10):
            result += int(binary_output[0][i]) * (2 ** (9-i))
        return result

if __name__ == "__main__":
    # GPU ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # 2ë§Œê°œ ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ (ìƒì¡´ë¹„ìœ¨ 0.01~0.20)
    data_file_small = [
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.010000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.020000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.030000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.040000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.050000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.060000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.070000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.080000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.090000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.100000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.110000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.120000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.130000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.140000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.150000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.160000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.170000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.180000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.190000.txt",
        "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.200000.txt"
    ]
    
    # 99ë§Œê°œ ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ (ìƒì¡´ë¹„ìœ¨ 0.01~0.99)
    data_file_full = []
    for i in range(1, 100):  # 0.01ë¶€í„° 0.99ê¹Œì§€
        ratio = i / 100.0
        filename = f"/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_{ratio:.6f}.txt"
        data_file_full.append(filename)
    
    # ë°ì´í„°ì…‹ ì„ íƒ
    print("\nì‚¬ìš©í•  ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ì†Œí˜• ë°ì´í„°ì…‹ (2ë§Œê°œ, ìƒì¡´ë¹„ìœ¨ 0.01~0.20)")
    print("2. ì „ì²´ ë°ì´í„°ì…‹ (9.9ë§Œê°œ, ìƒì¡´ë¹„ìœ¨ 0.01~0.99)")
    
    dataset_choice = input("ì„ íƒ (1 ë˜ëŠ” 2): ").strip()
    
    if dataset_choice == "2":
        print("ğŸš€ ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
        train_loader, dataset = create_dataloader_from_files(data_file_full, batch_size=100)
        dataset_name = "99ë§Œê°œ ì „ì²´ ë°ì´í„°ì…‹"
    else:
        print("ğŸ“Š ì†Œí˜• ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
        train_loader, dataset = create_dataloader_from_files(data_file_small, batch_size=100)
        dataset_name = "2ë§Œê°œ ì†Œí˜• ë°ì´í„°ì…‹"
    
    if train_loader is None or dataset is None:
        print("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
        exit()

    # í›ˆë ¨ ëª¨ë“œ ì„ íƒ
    mode = input("\ní›ˆë ¨ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:\n1. ìƒˆ ëª¨ë¸ í›ˆë ¨\n2. ê¸°ì¡´ ëª¨ë¸ ì¶”ê°€ í›ˆë ¨\nì„ íƒ (1 ë˜ëŠ” 2): ").strip()
    
    if mode == "2":
        # ê¸°ì¡´ ëª¨ë¸ ì¶”ê°€ í›ˆë ¨
        model_path = input("ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: saved_models/cnn_gol_model9.pth): ").strip()
        if not model_path:
            model_path = "saved_models/cnn_gol_model9.pth"
        
        additional_epochs = int(input("ì¶”ê°€ í›ˆë ¨í•  ì—í­ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 100): ") or "100")
        
        # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ì—¬ë¶€ ì„ íƒ
        reset_opt = input("ì˜µí‹°ë§ˆì´ì €ë¥¼ ìƒˆë¡œ ì´ˆê¸°í™”í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").strip().lower()
        reset_optimizer = reset_opt != 'n'  # ê¸°ë³¸ê°’ì€ True
        
        # ìƒˆë¡œìš´ í•™ìŠµë¥  ì„¤ì • (ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ì‹œ)
        if reset_optimizer:
            new_lr = float(input("ìƒˆ í•™ìŠµë¥ ì„ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 0.0001): ") or "0.0001")
        else:
            new_lr = 0.001
        
        print(f"\nğŸ”„ ê¸°ì¡´ ëª¨ë¸ ì¶”ê°€ í›ˆë ¨: {model_path}")
        
        # ë°ì´í„°ì…‹ ì •ë³´ ì¤€ë¹„
        if dataset is not None:
            dataset_info = {
                'samples': len(dataset),
                'label_range': (min(dataset.labels), max(dataset.labels)),
                'dataset_type': dataset_name
            }
            print(f"ğŸ“ˆ ì‚¬ìš© ë°ì´í„°: {dataset_info['samples']}ê°œ ìƒ˜í”Œ, ë ˆì´ë¸” ë²”ìœ„: {dataset_info['label_range'][0]}~{dataset_info['label_range'][1]}")
        else:
            dataset_info = None
        
        result = continue_training(model_path, train_loader, additional_epochs, lr=new_lr, reset_optimizer=reset_optimizer, dataset_info=dataset_info)
        
        if result[0] is not None and result[1] is not None:
            model, new_path = result
            print(f"ì¶”ê°€ í›ˆë ¨ ì™„ë£Œ! ìƒˆ ëª¨ë¸ ì €ì¥: {new_path}")
            
            # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
            if dataset is not None and len(dataset) > 0:
                test_data = dataset[0][0].unsqueeze(0)
                predicted_number = predict_to_number(model, test_data)
                print(f'í…ŒìŠ¤íŠ¸ - ì‹¤ì œ: {dataset.labels[0]}, ì˜ˆì¸¡: {predicted_number}')
        else:
            print("ì¶”ê°€ í›ˆë ¨ ì‹¤íŒ¨!")
    
    else:
        # ìƒˆ ëª¨ë¸ í›ˆë ¨
        print("\nğŸ†• ìƒˆ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        print(f"ğŸ“Š {dataset_name} ì‚¬ìš©")
        
        model = cnnLayer(
            inputSize=10,
            hidden1Size=32,
            hidden2Size=64,
            outputSize=10,
            activate='swish',
            stride=1,
            use_bias=False
        ).to(device)
        
        epochs = int(input("í›ˆë ¨í•  ì—í­ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 500): ") or "500")
        
        import time
        start_time = time.time()
        
        # ì˜µí‹°ë§ˆì´ì € ìƒì„± (ì €ì¥ìš©)
        optimizer = optim.AdamW(model.parameters(), lr=0.001)
        
        # í›ˆë ¨
        train_model(model, train_loader, epochs=epochs)
        
        end_time = time.time()
        print(f"í›ˆë ¨ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")

        model_info = {
            'input_size': 10,
            'hidden1_size': 32,
            'hidden2_size': 64,
            'output_size': 10,
            'activate': 'swish',
            'stride': 1,
            'training_samples': len(dataset) if dataset else 0,
            'training_epochs': epochs,
            'label_range': (min(dataset.labels), max(dataset.labels)) if dataset else (0, 0),
            'dataset_type': dataset_name
        }
        
        # ëª¨ë¸ ì €ì¥ (ì˜µí‹°ë§ˆì´ì € ìƒíƒœ í¬í•¨)
        if dataset_choice == "2":
            model_path = "saved_models/cnn_gol_model_full99k.pth"
        else:
            model_path = "saved_models/cnn_gol_model_small20k.pth"
        save_model(model, model_path, model_info, optimizer, epochs)
        
        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
        if dataset is not None and len(dataset) > 0:
            test_data = dataset[0][0].unsqueeze(0)
            predicted_number = predict_to_number(model, test_data)
            print(f'í…ŒìŠ¤íŠ¸ - ì‹¤ì œ: {dataset.labels[0]}, ì˜ˆì¸¡: {predicted_number}')