import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
from cuda_pattern_dataset import CUDAPatternDataset, create_cuda_pattern_dataloader

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

def getActive(activeName, input):
    if activeName == 'tanh':
        return F.tanh(input)
    elif activeName == 'relu':
        return F.relu(input)
    elif activeName == 'sigmoid':
        return F.sigmoid(input)
    elif activeName == 'elu':
        return F.elu(input)
    elif activeName == 'selu':
        return F.selu(input)
    elif activeName == 'lrelu':
        return F.leaky_relu(input)
    elif activeName == 'swish':
        return F.silu(input)
    else:
        return F.tanh(input)

class DenseLayer(nn.Module):
    def __init__(self, inputLayer, hidden1Layer, hidden2Layer, hidden3Layer, outputLayer, activate):
        super(DenseLayer, self).__init__()
        self.fc1 = nn.Linear(inputLayer, hidden1Layer)
        self.fc2 = nn.Linear(hidden1Layer, hidden2Layer)
        self.fc3 = nn.Linear(hidden2Layer, hidden3Layer)
        self.fc4 = nn.Linear(hidden3Layer, outputLayer)
        self.fc_act = activate

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = getActive(self.fc_act, self.fc1(x))
        x = getActive(self.fc_act, self.fc2(x))
        x = getActive(self.fc_act, self.fc3(x))
        x = torch.sigmoid(self.fc4(x))
        return x

class cnnLayer(nn.Module):
    def __init__(self, inputSize, hidden1Size, hidden2Size, outputSize, activate, stride, use_bias):
        super(cnnLayer, self).__init__()
        
        # CNN ë ˆì´ì–´ë“¤ (3ê°œ)
        self.conv1 = nn.Conv2d(1, hidden1Size, kernel_size=3, stride=stride, padding=1, bias=use_bias)
        self.conv2 = nn.Conv2d(hidden1Size, hidden2Size, kernel_size=3, stride=stride, padding=1, bias=use_bias)
        self.conv3 = nn.Conv2d(hidden2Size, hidden2Size*2, kernel_size=3, stride=stride, padding=1, bias=use_bias)
        
        # í’€ë§ ë ˆì´ì–´
        self.pool = nn.MaxPool2d(2, 2)
        
        # ë°°ì¹˜ ì •ê·œí™”
        self.bn1 = nn.BatchNorm2d(hidden1Size, affine=False)
        self.bn2 = nn.BatchNorm2d(hidden2Size, affine=False)
        self.bn3 = nn.BatchNorm2d(hidden2Size*2, affine=False)
        
        # FC layer ì…ë ¥ í¬ê¸° ê³„ì‚°
        self._calculate_fc_input_size(inputSize, hidden2Size*2)
        
        # Dense ë ˆì´ì–´ë“¤ (5ê°œ)
        self.fc1 = nn.Linear(self.fc_input_size, 1024, bias=use_bias)
        self.fc2 = nn.Linear(1024, 512, bias=use_bias)
        self.fc3 = nn.Linear(512, 256, bias=use_bias)
        self.fc4 = nn.Linear(256, 128, bias=use_bias)
        self.fc5 = nn.Linear(128, outputSize, bias=use_bias)
        
        # í™œì„±í™” í•¨ìˆ˜ ì €ì¥
        self.fc_act = activate
    
    def _calculate_fc_input_size(self, inputSize, final_channels):
        """FC layer ì…ë ¥ í¬ê¸° ê³„ì‚°"""
        # 10x10 â†’ pool â†’ 5x5 â†’ pool â†’ 2x2 â†’ pool â†’ 1x1
        # 3ë²ˆì˜ pooling (2x2)ìœ¼ë¡œ 8ë°° ê°ì†Œ
        size_after_conv = max(1, inputSize // 8)
        self.fc_input_size = final_channels * size_after_conv * size_after_conv
    
    def forward(self, x):
        # ì…ë ¥ í˜•íƒœ ì¡°ì •: (batch, height, width) â†’ (batch, 1, height, width)
        if x.dim() == 3:
            x = x.unsqueeze(1)
        
        # CNN ë ˆì´ì–´ë“¤ (3ê°œ)
        x = self.pool(getActive(self.fc_act, self.bn1(self.conv1(x))))
        x = self.pool(getActive(self.fc_act, self.bn2(self.conv2(x))))
        x = self.pool(getActive(self.fc_act, self.bn3(self.conv3(x))))
        
        # Flatten
        x = x.view(x.size(0), -1)
        
        # Dense ë ˆì´ì–´ë“¤ (5ê°œ)
        x = getActive(self.fc_act, self.fc1(x))
        x = getActive(self.fc_act, self.fc2(x))
        x = getActive(self.fc_act, self.fc3(x))
        x = getActive(self.fc_act, self.fc4(x))
        x = torch.sigmoid(self.fc5(x))  # ë§ˆì§€ë§‰ ë ˆì´ì–´ëŠ” í™œì„±í™” í•¨ìˆ˜ ì ìš© ì•ˆ í•¨
        
        return x

def save_model(model, filepath, model_info=None, optimizer=None, epoch=0):
    """ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ ì •ë³´ ì €ì¥ (ì˜µí‹°ë§ˆì´ì € ìƒíƒœ í¬í•¨)"""
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    save_dict = {
        'model_state_dict': model.state_dict(),
        'model_info': model_info or {},
        'epoch': epoch
    }
    
    # ì˜µí‹°ë§ˆì´ì € ìƒíƒœë„ ì €ì¥ (ì¶”ê°€ í•™ìŠµìš©)
    if optimizer is not None:
        save_dict['optimizer_state_dict'] = optimizer.state_dict()
    
    torch.save(save_dict, filepath)
    print(f"ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")

def load_model(model_path, model_class, model_params, load_optimizer=False, lr=0.001):
    """ëª¨ë¸ ë¡œë“œ (ì¶”ê°€ í•™ìŠµ ì§€ì›)"""
    try:
        if torch.cuda.is_available():
            checkpoint = torch.load(model_path)
        else:
            checkpoint = torch.load(model_path, map_location='cpu')
        
        # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        model_info = checkpoint.get('model_info', {})
        saved_epoch = checkpoint.get('epoch', 0)
        
        # ëª¨ë¸ ìƒì„±
        model = model_class(**model_params).to(device)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        optimizer = optim.AdamW(model.parameters(), lr=lr)
        if load_optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print(f"ì˜µí‹°ë§ˆì´ì € ìƒíƒœë„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤ (Epoch {saved_epoch}ë¶€í„° ê³„ì†)")
        
        print(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
        print(f"ëª¨ë¸ ì •ë³´: {model_info}")
        print(f"ì €ì¥ëœ Epoch: {saved_epoch}")
        
        return model, optimizer, model_info, saved_epoch
        
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None, None, 0

def continue_training(model_path, train_loader, additional_epochs=100, lr=0.001, reset_optimizer=True, dataset_info=None):
    """ê¸°ì¡´ ëª¨ë¸ì„ ë¡œë“œí•´ì„œ ì¶”ê°€ í›ˆë ¨"""
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ì˜ (ì €ì¥ëœ ì •ë³´ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŒ)
    model_params = {
        'inputSize': 10,
        'hidden1Size': 32,
        'hidden2Size': 64,
        'outputSize': 10,
        'activate': 'swish',
        'stride': 1,
        'use_bias': False
    }
    
    # ëª¨ë¸ ë¡œë“œ (ì˜µí‹°ë§ˆì´ì €ëŠ” ì¡°ê±´ë¶€)
    model, optimizer, model_info, start_epoch = load_model(
        model_path, cnnLayer, model_params, load_optimizer=not reset_optimizer, lr=lr
    )
    
    if model is None:
        print("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨.")
        return None, None
    
    # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” (ìƒˆë¡œìš´ í•™ìŠµë¥ ë¡œ)
    if reset_optimizer:
        optimizer = optim.AdamW(model.parameters(), lr=lr)
        print(f"ğŸ”„ ì˜µí‹°ë§ˆì´ì € ìƒˆë¡œ ì´ˆê¸°í™” (í•™ìŠµë¥ : {lr})")
    elif optimizer is None:
        # ì˜µí‹°ë§ˆì´ì € ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìƒˆë¡œ ìƒì„±
        optimizer = optim.AdamW(model.parameters(), lr=lr)
        print(f"âš ï¸ ì˜µí‹°ë§ˆì´ì € ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„± (í•™ìŠµë¥ : {lr})")
    
    print(f"\nğŸ”„ ì¶”ê°€ í›ˆë ¨ ì‹œì‘: Epoch {start_epoch+1} ~ {start_epoch+additional_epochs}")
    
    # ì¶”ê°€ í›ˆë ¨
    criterion = nn.BCELoss()
    
    model.train()
    for epoch in range(additional_epochs):
        current_epoch = start_epoch + epoch + 1
        total_loss = 0
        
        for batch_x, batch_y in train_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        if (epoch + 1) % 20 == 0:
            print(f'Epoch [{current_epoch}/{start_epoch+additional_epochs}], Loss: {total_loss/len(train_loader):.4f}')
    
    # ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ ì •ë³´
    if model_info is not None:
        updated_info = model_info.copy()
        updated_info['total_epochs'] = start_epoch + additional_epochs
        updated_info['additional_training'] = True
        
        # ë°ì´í„°ì…‹ ì •ë³´ ì—…ë°ì´íŠ¸ (ì „ë‹¬ë°›ì€ ê²½ìš°)
        if dataset_info is not None:
            updated_info['training_samples'] = dataset_info['samples']
            updated_info['label_range'] = dataset_info['label_range']
            updated_info['dataset_type'] = dataset_info['dataset_type']
            
    else:
        updated_info = {'total_epochs': start_epoch + additional_epochs, 'additional_training': True}
        if dataset_info is not None:
            updated_info.update(dataset_info)
    
    # ìƒˆ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥ (ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸° ë°©ì§€)
    new_model_path = model_path.replace('.pth', f'_continued_{start_epoch+additional_epochs}.pth')
    save_model(model, new_model_path, updated_info, optimizer, start_epoch + additional_epochs)
    
    return model, new_model_path

class NewBinaryDataset(Dataset):
    def __init__(self, data_file):
        self.data = []
        self.labels = []
        self.load_data(data_file)
    
    def load_data(self, data_file):
        """ìƒˆë¡œìš´ í˜•ì‹ì˜ ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            i = 0
            sample_count = 0
            
            while i < len(lines):
                line = lines[i].strip()
                
                # [ìˆ«ì] í˜•ì‹ì˜ ìƒ˜í”Œ ì‹œì‘ í™•ì¸
                if line.startswith('[') and line.endswith(']'):
                    try:
                        # ìƒ˜í”Œ ë²ˆí˜¸ ì¶”ì¶œ
                        sample_num = int(line[1:-1])
                        
                        # 10x10 íŒ¨í„´ ë°ì´í„° ì½ê¸°
                        pattern_lines = []
                        for j in range(1, 11):  # ë‹¤ìŒ 10ì¤„
                            if i + j < len(lines):
                                pattern_line = lines[i + j].strip()
                                if len(pattern_line) == 10 and all(c in '01' for c in pattern_line):
                                    pattern_lines.append([int(bit) for bit in pattern_line])
                                else:
                                    break
                        
                        # ë ˆì´ë¸” ì½ê¸° (11ë²ˆì§¸ ì¤„)
                        if i + 11 < len(lines) and len(pattern_lines) == 10:
                            label_line = lines[i + 11].strip()
                            if label_line.isdigit():
                                label = int(label_line)
                                
                                # ë°ì´í„° ì €ì¥
                                self.data.append(pattern_lines)
                                self.labels.append(label)
                                sample_count += 1
                        
                        i += 12  # ë‹¤ìŒ ìƒ˜í”Œë¡œ ì´ë™ ([n] + 10ì¤„ íŒ¨í„´ + 1ì¤„ ë ˆì´ë¸”)
                        
                    except ValueError:
                        i += 1
                else:
                    i += 1
            
            print(f"Successfully loaded {sample_count} samples")
            if self.labels:
                print(f"Label range: {min(self.labels)} ~ {max(self.labels)}")
                
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        # 10x10 íŒ¨í„´ì„ í…ì„œë¡œ ë³€í™˜
        data = torch.tensor(self.data[idx], dtype=torch.float32)
        
        # 10bit ë³€í™˜
        label_value = self.labels[idx]
        label_10bit = [(label_value >> i) & 1 for i in range(9, -1, -1)]
        label = torch.tensor(label_10bit, dtype=torch.float32)
        
        return data, label

def create_dataloader_from_file(data_file, batch_size=64, shuffle=True, num_workers=4):
    """íŒŒì¼ì—ì„œ ì§ì ‘ ë°ì´í„°ë¡œë” ìƒì„±"""
    dataset = NewBinaryDataset(data_file)
    
    if len(dataset) == 0:
        print("ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return None, None
    
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)
    
    return dataloader, dataset

class BinaryDataset(Dataset):
    # ... ê¸°ì¡´ ì½”ë“œ ë™ì¼ ...
    def __init__(self, data_dir):
        self.data = []
        self.labels = []
        self.load_data(data_dir)
    
    def load_data(self, data_dir):
        for filename in os.listdir(data_dir):
            if filename.endswith('.txt'):
                filepath = os.path.join(data_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    if len(lines) < 11:
                        continue
                    
                    binary_patterns = []
                    for i in range(10):
                        line = lines[i].strip()
                        if line and all(c in '01' for c in line):
                            binary_pattern = [int(bit) for bit in line]
                            binary_patterns.append(binary_pattern)
                    
                    label_line = lines[10].strip()
                    if label_line.lstrip('-').isdigit():
                        label = int(label_line)
                    else:
                        continue
                    
                    if len(binary_patterns) == 10:
                        self.data.append(binary_patterns)
                        self.labels.append(label)
                        
                except Exception as e:
                    continue
        
        print(f"Successfully loaded {len(self.data)} samples")
        if self.labels:
            print(f"Label range: {min(self.labels)} ~ {max(self.labels)}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        data = torch.tensor(self.data[idx], dtype=torch.float32)
        
        label_value = self.labels[idx]
        label_10bit = [(label_value >> i) & 1 for i in range(9, -1, -1)]
        label = torch.tensor(label_10bit, dtype=torch.float32)
        
        return data, label

def create_dataloader(data_dir, batch_size=32, shuffle=True, num_workers=4):
    dataset = BinaryDataset(data_dir)
    if len(dataset) == 0:
        return None
    # num_workers ì¶”ê°€ë¡œ ë°ì´í„° ë¡œë”© ì†ë„ í–¥ìƒ
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, 
                          num_workers=num_workers, pin_memory=True)
    return dataloader

def train_model(model, train_loader, epochs=100):
    criterion = nn.BCELoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.001)
    
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        if (epoch + 1) % 20 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')


def predict_to_number(model, data_tensor):
    model.eval()
    with torch.no_grad():
        # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
        data_tensor = data_tensor.to(device)
        output = model(data_tensor)
        binary_output = (output > 0.5).float()
        # GPUì—ì„œ CPUë¡œ ì´ë™ í›„ ê³„ì‚°
        binary_output = binary_output.cpu()
        result = 0
        for i in range(10):
            result += int(binary_output[0][i]) * (2 ** (9-i))
        return result

class MultiFileDataset(Dataset):
    def __init__(self, file_list):
        self.data = []
        self.labels = []
        self.file_info = []
        self.load_multiple_files(file_list)
    
    def load_multiple_files(self, file_list):
        """ì—¬ëŸ¬ íŒŒì¼ì„ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ í•©ì¹¨"""
        total_samples = 0
        
        for file_path in file_list:
            if os.path.exists(file_path):
                file_data, file_labels = self.load_single_file(file_path)
                
                # íŒŒì¼ë³„ í†µê³„
                if file_data:
                    survival_ratio = self.extract_survival_ratio(file_path)
                    self.file_info.append({
                        'path': file_path,
                        'samples': len(file_data),
                        'survival_ratio': survival_ratio,
                        'label_range': (min(file_labels), max(file_labels))
                    })
                    
                    self.data.extend(file_data)
                    self.labels.extend(file_labels)
                    total_samples += len(file_data)
                    
                    print(f"âœ… {os.path.basename(file_path)}: {len(file_data)}ê°œ ìƒ˜í”Œ, ë ˆì´ë¸” ë²”ìœ„: {min(file_labels)}~{max(file_labels)}")
            else:
                print(f"âŒ íŒŒì¼ ì—†ìŒ: {file_path}")
        
        print(f"\nğŸ“Š ì´ ë¡œë“œëœ ìƒ˜í”Œ: {total_samples}ê°œ")
        if self.labels:
            print(f"ğŸ“Š ì „ì²´ ë ˆì´ë¸” ë²”ìœ„: {min(self.labels)} ~ {max(self.labels)}")
    
    def load_single_file(self, data_file):
        """ë‹¨ì¼ íŒŒì¼ ë¡œë“œ"""
        data = []
        labels = []
        
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            i = 0
            while i < len(lines):
                line = lines[i].strip()
                
                if line.startswith('[') and line.endswith(']'):
                    try:
                        # 10x10 íŒ¨í„´ ì½ê¸°
                        pattern_lines = []
                        for j in range(1, 11):
                            if i + j < len(lines):
                                pattern_line = lines[i + j].strip()
                                if len(pattern_line) == 10 and all(c in '01' for c in pattern_line):
                                    pattern_lines.append([int(bit) for bit in pattern_line])
                        
                        # ë ˆì´ë¸” ì½ê¸°
                        if i + 11 < len(lines) and len(pattern_lines) == 10:
                            label_line = lines[i + 11].strip()
                            if label_line.isdigit():
                                label = int(label_line)
                                data.append(pattern_lines)
                                labels.append(label)
                        
                        i += 12
                    except (ValueError, IndexError):
                        i += 1
                else:
                    i += 1
                    
        except Exception as e:
            print(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {data_file}: {e}")
        
        return data, labels
    
    def extract_survival_ratio(self, file_path):
        """íŒŒì¼ëª…ì—ì„œ ìƒì¡´ ë¹„ìœ¨ ì¶”ì¶œ"""
        try:
            # database-12345_100_0.030000.txt â†’ 0.03
            parts = os.path.basename(file_path).split('_')
            ratio_part = parts[-1].replace('.txt', '')
            return float(ratio_part)
        except:
            return 0.0
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        # 10x10 íŒ¨í„´ì„ í…ì„œë¡œ ë³€í™˜
        data = torch.tensor(self.data[idx], dtype=torch.float32)
        
        # 10bit ë³€í™˜
        label_value = self.labels[idx]
        label_10bit = [(label_value >> i) & 1 for i in range(9, -1, -1)]
        label = torch.tensor(label_10bit, dtype=torch.float32)
        
        return data, label
    
    def print_dataset_info(self):
        """ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ë°ì´í„°ì…‹ ë¶„ì„ ê²°ê³¼")
        print("="*60)
        
        for info in self.file_info:
            print(f"ìƒì¡´ë¹„ìœ¨ {info['survival_ratio']:.2f}: {info['samples']:3d}ê°œ, ë ˆì´ë¸” {info['label_range'][0]:3d}~{info['label_range'][1]:3d}")
        
        print(f"\nì´ íŒŒì¼ ìˆ˜: {len(self.file_info)}ê°œ")
        print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(self.data)}ê°œ")
        print(f"ë ˆì´ë¸” ë²”ìœ„: {min(self.labels)} ~ {max(self.labels)}")

def create_multifile_dataloader(file_list, batch_size=64, shuffle=True, num_workers=4):
    """ë©€í‹° íŒŒì¼ ë°ì´í„°ë¡œë” ìƒì„±"""
    dataset = MultiFileDataset(file_list)
    
    if len(dataset) == 0:
        print("âŒ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return None, None
    
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,
                          num_workers=num_workers, pin_memory=True)
    
    return dataloader, dataset



if __name__ == "__main__":
    # GPU ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    data_file = "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-12345_600000_0.300000.txt"

    data_file_testing = ["/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.010000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.020000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.030000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.040000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.050000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.060000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.070000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.080000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.090000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.100000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.110000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.120000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.130000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.140000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.150000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.160000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.170000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.180000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.190000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.200000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.210000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.220000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.230000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.240000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.250000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.260000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.270000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.280000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.290000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.300000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.310000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.320000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.330000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.340000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.350000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.360000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.370000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.380000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.390000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.400000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.410000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.420000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.430000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.440000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.450000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.460000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.470000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.480000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.490000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.500000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.510000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.520000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.530000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.540000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.550000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.560000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.570000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.580000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.590000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.600000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.610000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.620000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.630000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.640000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.650000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.660000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.670000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.680000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.690000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.700000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.710000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.720000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.730000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.740000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.750000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.760000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.770000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.780000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.790000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.800000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.810000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.820000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.830000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.840000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.850000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.860000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.870000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.880000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.890000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.900000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.910000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.920000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.930000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.940000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.950000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.960000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.970000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.980000.txt",
                         "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.990000.txt"]
    
    data_file_for_finetune = ["/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.010000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.020000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.030000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.040000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.050000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.060000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.070000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.080000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.090000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.100000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.110000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.120000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.130000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.140000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.150000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.160000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.170000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.180000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.190000.txt",
                              "/home/sjh100/ë°”íƒ•í™”ë©´/pracPyTorch/train_data/database-54321_1000_0.200000.txt"]
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader, dataset = create_multifile_dataloader(data_file_testing, batch_size=100)
    train_loader2, dataset2 = create_dataloader_from_file(data_file, batch_size=100)
    train_loader_fine, dataset_fine = create_dataloader_from_file(data_file_for_finetune, batch_size=100)
    
    # CUDA íŒ¨í„´ ë°ì´í„°ë¡œë” (ìƒˆë¡œ ì¶”ê°€)
    train_loader_patterns, dataset_patterns = create_cuda_pattern_dataloader(
        num_samples=2600,  # 13ê°œ íŒ¨í„´ Ã— 200ê°œì”©
        include_patterns=None,  # ëª¨ë“  íŒ¨í„´ ì‚¬ìš©
        batch_size=100,
        random_positions=True,
        random_rotations=True,
        seed=42
    )
    
    if train_loader is None or dataset is None:
        print("ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
        exit()
    print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
    
    # ë°ì´í„°ì…‹ í†µê³„ ì¶œë ¥
    if train_loader_patterns is not None and dataset_patterns is not None:
        print(f"\nğŸ® CUDA íŒ¨í„´ ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: {len(dataset_patterns)}ê°œ ìƒ˜í”Œ")
        dataset_patterns.print_statistics()
    
    # í›ˆë ¨ ëª¨ë“œ ì„ íƒ
    mode = input("\ní›ˆë ¨ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:\n1. ìƒˆ ëª¨ë¸ í›ˆë ¨\n2. ê¸°ì¡´ ëª¨ë¸ ì¶”ê°€ í›ˆë ¨\nì„ íƒ (1 ë˜ëŠ” 2): ").strip()
    
    if mode == "2":
        # ê¸°ì¡´ ëª¨ë¸ ì¶”ê°€ í›ˆë ¨
        model_path = input("ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: saved_models/cnn_gol_model6.pth): ").strip()
        if not model_path:
            model_path = "saved_models/cnn_gol_model6_exp.pth"
        
        additional_epochs = int(input("ì¶”ê°€ í›ˆë ¨í•  ì—í­ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 100): ") or "100")
        
        # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ì—¬ë¶€ ì„ íƒ
        reset_opt = input("ì˜µí‹°ë§ˆì´ì €ë¥¼ ìƒˆë¡œ ì´ˆê¸°í™”í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").strip().lower()
        reset_optimizer = reset_opt != 'n'  # ê¸°ë³¸ê°’ì€ True
        
        # ìƒˆë¡œìš´ í•™ìŠµë¥  ì„¤ì • (ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ì‹œ)
        if reset_optimizer:
            new_lr = float(input("ìƒˆ í•™ìŠµë¥ ì„ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 0.0001): ") or "0.0001")
        else:
            new_lr = 0.001
        
        # ë°ì´í„°ì…‹ ì„ íƒ
        print("\nì‚¬ìš©í•  ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. 10ë§Œê°œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹")
        print("2. 2ë§Œê°œ ë‹¤ì–‘ì„± ë°ì´í„°ì…‹")
        print("3. CUDA íŒ¨í„´ ë°ì´í„°ì…‹ (2600ê°œ)")
        
        dataset_choice = input("ì„ íƒ (1, 2, ë˜ëŠ” 3): ").strip()
        
        if dataset_choice == "1" and train_loader2 is not None and dataset2 is not None:
            selected_loader = train_loader2
            selected_dataset = dataset2
            print("ğŸš€ 60ë§Œê°œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ìœ¼ë¡œ ì¶”ê°€ í›ˆë ¨")
        elif dataset_choice == "3" and train_loader_patterns is not None and dataset_patterns is not None:
            selected_loader = train_loader_patterns
            selected_dataset = dataset_patterns
            print("ğŸ® CUDA íŒ¨í„´ ë°ì´í„°ì…‹ìœ¼ë¡œ ì¶”ê°€ í›ˆë ¨")
        else:
            selected_loader = train_loader_fine
            selected_dataset = dataset_fine
            print("ğŸ“Š 2ë§Œê°œ ë‹¤ì–‘ì„± ë°ì´í„°ì…‹ìœ¼ë¡œ ì¶”ê°€ í›ˆë ¨")
        
        print(f"\nğŸ”„ ê¸°ì¡´ ëª¨ë¸ ì¶”ê°€ í›ˆë ¨: {model_path}")
        
        # ë°ì´í„°ì…‹ ì •ë³´ ì¤€ë¹„
        dataset_info = None
        if selected_dataset is not None:
            dataset_info = {
                'samples': len(selected_dataset),
                'label_range': (min(selected_dataset.labels), max(selected_dataset.labels)),
                'dataset_type': 'ëŒ€ìš©ëŸ‰ 60ë§Œê°œ' if selected_dataset == dataset2 else 'ë‹¤ì–‘ì„± 2800ê°œ'
            }
            print(f"ğŸ“ˆ ì‚¬ìš© ë°ì´í„°: {dataset_info['samples']}ê°œ ìƒ˜í”Œ, ë ˆì´ë¸” ë²”ìœ„: {dataset_info['label_range'][0]}~{dataset_info['label_range'][1]}")
        
        result = continue_training(model_path, selected_loader, additional_epochs, lr=new_lr, reset_optimizer=reset_optimizer, dataset_info=dataset_info)
        
        if result[0] is not None and result[1] is not None:
            model, new_path = result
            print(f"ì¶”ê°€ í›ˆë ¨ ì™„ë£Œ! ìƒˆ ëª¨ë¸ ì €ì¥: {new_path}")
            
            # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
            test_data = dataset[0][0].unsqueeze(0)
            predicted_number = predict_to_number(model, test_data)
            print(f'í…ŒìŠ¤íŠ¸ - ì‹¤ì œ: {dataset.labels[0]}, ì˜ˆì¸¡: {predicted_number}')
        else:
            print("ì¶”ê°€ í›ˆë ¨ ì‹¤íŒ¨!")
    
    else:
        # ìƒˆ ëª¨ë¸ í›ˆë ¨
        print("\nğŸ†• ìƒˆ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        # ë°ì´í„°ì…‹ ì„ íƒ
        print("\nì‚¬ìš©í•  ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. 9ë§Œê°œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (ê¸°ë³¸)")
        print("2. 60ë§Œê°œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹")
        print("3. CUDA íŒ¨í„´ ë°ì´í„°ì…‹ (2600ê°œ)")
        
        dataset_choice = input("ì„ íƒ (1, 2, ë˜ëŠ” 3): ").strip()
        
        if dataset_choice == "2" and train_loader2 is not None and dataset2 is not None:
            selected_train_loader = train_loader2
            selected_train_dataset = dataset2
            print("ğŸš€ 60ë§Œê°œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ìœ¼ë¡œ ìƒˆ ëª¨ë¸ í›ˆë ¨")
        elif dataset_choice == "3" and train_loader_patterns is not None and dataset_patterns is not None:
            selected_train_loader = train_loader_patterns
            selected_train_dataset = dataset_patterns
            print("ğŸ® CUDA íŒ¨í„´ ë°ì´í„°ì…‹ìœ¼ë¡œ ìƒˆ ëª¨ë¸ í›ˆë ¨")
        else:
            selected_train_loader = train_loader
            selected_train_dataset = dataset
            print("ğŸ“Š 9ë§Œê°œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ìƒˆ ëª¨ë¸ í›ˆë ¨")
        
        model = cnnLayer(
            inputSize=10,
            hidden1Size=32,
            hidden2Size=64,
            outputSize=10,
            activate='swish',
            stride=1,
            use_bias=False
        ).to(device)
        
        epochs = int(input("í›ˆë ¨í•  ì—í­ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 500): ") or "500")
        
        import time
        start_time = time.time()
        
        # ì˜µí‹°ë§ˆì´ì € ìƒì„± (ì €ì¥ìš©)
        optimizer = optim.AdamW(model.parameters(), lr=0.001)
        
        # ì„ íƒëœ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨
        train_model(model, selected_train_loader, epochs=epochs)
        
        end_time = time.time()
        print(f"í›ˆë ¨ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")

        model_info = {
            'input_size': 10,
            'hidden1_size': 32,
            'hidden2_size': 64,
            'output_size': 10,
            'activate': 'swish',
            'stride': 1,
            'training_samples': len(selected_train_dataset),
            'training_epochs': epochs,
            'label_range': (min(selected_train_dataset.labels), max(selected_train_dataset.labels)),
            'dataset_type': 'CUDAíŒ¨í„´' if selected_train_dataset == dataset_patterns else 'ì‹œë®¬ë ˆì´ì…˜'
        }
        
        # ëª¨ë¸ ì €ì¥ (ì˜µí‹°ë§ˆì´ì € ìƒíƒœ í¬í•¨)
        model_path = "saved_models/cnn_gol_model9.pth"
        save_model(model, model_path, model_info, optimizer, epochs)
        
        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
        test_data = selected_train_dataset[0][0].unsqueeze(0)
        predicted_number = predict_to_number(model, test_data)
        print(f'í…ŒìŠ¤íŠ¸ - ì‹¤ì œ: {selected_train_dataset.labels[0]}, ì˜ˆì¸¡: {predicted_number}')

